2nd Bigram Model
    |_ Vocab Embedding table (vocab_size, emb_dim)
    |_ Position Embedding table (Learned positional embedding) (context_size, emb_dim)
    |_ Linear (emb_dim, vocab_size)

    Final loss: 2.42 (1480 steps, avg every 10 steps)

    1480 steps because loss peaked after that. I'm running 1500 steps and cutting the final peak.