2nd Bigram Model
    |_ Vocab Embedding table (vocab_size, emb_dim)
    |_ Position Embedding table (Learned positional embedding) (context_size, emb_dim)
    |_ ScaledDotProdSelfAtt (will keep the input dim)
    |_ Linear (emb_dim, vocab_size)

    Final losses: train 2.3271; eval 2.3248(10,000 steps, eval for 100 steps every 100 steps, batch 32, context 32, emb dim 32)

    Looks about the result as v2 ðŸ¤”

    Example Generation:
        cod niede eve ditlet mang yofo thy
        SThind mean te url hat ght me machan fl haisu disere----wurore cense wn ch me, womes thee?
        Pma, meth 'll me hern an he wlars ount-
        f ind s heve she, nd bran ndowe:
        L