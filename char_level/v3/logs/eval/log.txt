2nd Bigram Model
    |_ Vocab Embedding table (vocab_size, emb_dim)
    |_ Position Embedding table (Learned positional embedding) (context_size, emb_dim)
    |_ AvgSelfAtt (will keep the input dim)
    |_ Linear (emb_dim, vocab_size)

    Final losses: train 3.0324; eval 3.0270 (10,000 steps, eval for 100 steps every 100 steps, batch 32, context 32, emb dim 32)

    Average Self Attention made it worst ðŸ˜¢

    Example Generation:
        Toitn,ede ei Rdf lat mangEywih hps
        bshtaaamsa  er url hategpt mt machanreH .teen 
        hie:e-isbwu
        orhic 